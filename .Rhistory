V_t # rows of V^T
svd_result$d # singular values
# Describe spatial and temporal modes with their variances/energies
total_variance <- sum(svd_result$d^2)
mode_variances <- svd_result$d^2
mode_variances # Variance explained by each mode:
mode_energies <- mode_variances / total_variance
mode_energies # Energy (proportion of variance) explained by each mode:
# Interpretation (to be written in comments or printed)
cat("\nInterpretation:\n")
cat("1. Spatial Modes:\n")
cat("   - Mode 1:", svd_result$u[,1], "\n")
cat("   - Mode 2:", svd_result$u[,2], "\n")
cat("2. Temporal Modes:\n")
cat("   - Mode 1:", V_t[1,], "\n")
cat("   - Mode 2:", V_t[2,], "\n")
cat("3. Variances:\n")
cat("   - Mode 1:", mode_variances[1], "\n")
cat("   - Mode 2:", mode_variances[2], "\n")
cat("4. Energies:\n")
cat("   - Mode 1:", mode_energies[1], "or", mode_energies[1]*100, "%\n")
cat("   - Mode 2:", mode_energies[2], "or", mode_energies[2]*100, "%\n")
# Exercise 1.2
# Load necessary libraries
library(tidyverse)
library(lubridate)
library(anomaly)
library(ggplot2)
plot(9,9,
main = 'An eigenvector vs a non-eigenvector',
cex.axis = 1.4, cex.lab = 1.4,
xlim = c(0,3), ylim=c(0,3),
xlab = bquote(x[1]), ylab = bquote(x[2]))
arrows(0,0, 1,0, length = 0.25,
angle = 8, lwd = 5, col = 'blue')
arrows(0,0, 1,2, length = 0.3,
angle = 8, lwd = 2, col = 'blue',  lty = 3)
arrows(0,0, 1,1, length = 0.25,
angle = 8, lwd = 5, col='red')
arrows(0,0, 3,3, length = 0.3,
angle = 8, lwd = 2, col='red', lty = 3)
text(1.4,0.1, 'Non-eigenvector u', cex =1.4, col = 'blue')
text(1.0,2.1, 'Au', cex =1.4, col = 'blue')
text(1.5,0.9, 'Eigenvector v', cex =1.4, col = 'red')
text(2.8, 2.95, 'Av', cex =1.4, col = 'red')
dev.off()
# Verify diagonalization and decomposition: R code
C = matrix(c(2,1,1,2), nrow = 2)
eigen(C)
#$values
#[1] 3 1
#$vectors
#  [,1]       [,2]
#[1,] 0.7071068 -0.7071068
#[2,] 0.7071068  0.7071068
Q = eigen(C)$vectors
D = t(Q)%*%C%*%Q #Matrix diagonalization
D
#[1,]    3    0
#[2,]    0    1
Q%*%D%*%t(Q) #Matrix decomposition
#[1,]    2    1
#[2,]    1    2
D[1,1]*Q[,1]%*%t(Q[,1]) + D[2,2]*Q[,2]%*%t(Q[,2])
# Verify diagonalization and decomposition: R code
C = matrix(c(2,1,1,2), nrow = 2)
eigen(C)
#$values
#[1] 3 1
#$vectors
#  [,1]       [,2]
#[1,] 0.7071068 -0.7071068
#[2,] 0.7071068  0.7071068
Q = eigen(C)$vectors
Q
D = t(Q)%*%C%*%Q #Matrix diagonalization
D
#[1,]    3    0
#[2,]    0    1
Q%*%D%*%t(Q) #Matrix decomposition
plot(9,9,
main = 'An eigenvector vs a non-eigenvector',
cex.axis = 1.4, cex.lab = 1.4,
xlim = c(0,3), ylim=c(0,3),
xlab = bquote(x[1]), ylab = bquote(x[2]))
arrows(0,0, 1,0, length = 0.25,
angle = 8, lwd = 5, col = 'blue')
arrows(0,0, 1,2, length = 0.3,
angle = 8, lwd = 2, col = 'blue',  lty = 3)
arrows(0,0, 1,1, length = 0.25,
angle = 8, lwd = 5, col='red')
arrows(0,0, 3,3, length = 0.3,
angle = 8, lwd = 2, col='red', lty = 3)
text(1.4,0.1, 'Non-eigenvector u', cex =1.4, col = 'blue')
text(1.0,2.1, 'Au', cex =1.4, col = 'blue')
text(1.5,0.9, 'Eigenvector v', cex =1.4, col = 'red')
plot(9,9,
main = 'An eigenvector vs a non-eigenvector',
cex.axis = 1.4, cex.lab = 1.4,
xlim = c(0,3), ylim=c(0,3),
xlab = bquote(x[1]), ylab = bquote(x[2]))
arrows(0,0, 1,0, length = 0.25,
angle = 8, lwd = 5, col = 'blue')
arrows(0,0, 1,2, length = 0.3,
angle = 8, lwd = 2, col = 'blue',  lty = 3)
arrows(0,0, 1,1, length = 0.25,
angle = 8, lwd = 5, col='red')
arrows(0,0, 3,3, length = 0.3,
angle = 8, lwd = 2, col='red', lty = 3)
text(1.4,0.1, 'Non-eigenvector u', cex =1.4, col = 'blue')
text(1.0,2.1, 'Au', cex =1.4, col = 'blue')
text(1.5,0.9, 'Eigenvector v', cex =1.4, col = 'red')
text(2.8, 2.95, 'Av', cex =1.4, col = 'red')
dev.off()
plot(9,9,
main = 'An eigenvector vs a non-eigenvector',
cex.axis = 1.4, cex.lab = 1.4,
xlim = c(-2,2), ylim=c(-2,2),
xlab = bquote(x[1]), ylab = bquote(x[2]))
arrows(0,0, Q[,1], length = 0.25,
angle = 8, lwd = 5, col = 'blue')
arrows(0,0, Q[,1][1], Q[,1][2], length = 0.25,
angle = 8, lwd = 5, col = 'blue')
plot(9,9,
main = 'An eigenvector vs a non-eigenvector',
cex.axis = 1.4, cex.lab = 1.4,
xlim = c(-2,2), ylim=c(-2,2),
xlab = bquote(x[1]), ylab = bquote(x[2]))
arrows(0,0, Q[,1][1], Q[,1][2], length = 0.25,
angle = 8, lwd = 5, col = 'blue')
plot(9,9,
main = 'An eigenvector vs a non-eigenvector',
cex.axis = 1.4, cex.lab = 1.4,
xlim = c(-2,2), ylim=c(-2,2),
xlab = bquote(x[1]), ylab = bquote(x[2]))
arrows(0,0, Q[,1][1], Q[,1][2], length = 0.25,
angle = 8, lwd = 5, col = 'blue')
arrows(0,0, Q[,2][1], Q[,2][2], length = 0.3,
angle = 8, lwd = 2, col = 'blue',  lty = 3)
plot(9,9,
main = 'An eigenvector vs a non-eigenvector',
cex.axis = 1.4, cex.lab = 1.4,
xlim = c(-1.2,1.2), ylim=c(-1.2,1.2),
xlab = bquote(x[1]), ylab = bquote(x[2]))
arrows(0,0, Q[,1][1], Q[,1][2], length = 0.25,
angle = 8, lwd = 5, col = 'blue')
arrows(0,0, Q[,2][1], Q[,2][2], length = 0.3,
angle = 8, lwd = 2, col = 'red',  lty = 3)
arrows(0,0, 1,1, length = 0.25,
angle = 8, lwd = 5, col='red')
arrows(0,0, 3,3, length = 0.3,
angle = 8, lwd = 2, col='red', lty = 3)
text(1.4,0.1, 'Non-eigenvector u', cex =1.4, col = 'blue')
text(1.0,2.1, 'Au', cex =1.4, col = 'blue')
plot(9,9,
main = 'An eigenvector vs a non-eigenvector',
cex.axis = 1.4, cex.lab = 1.4,
xlim = c(-1.2,1.2), ylim=c(-1.2,1.2),
xlab = bquote(x[1]), ylab = bquote(x[2]))
arrows(0,0, Q[,1][1], Q[,1][2], length = 0.25,
angle = 8, lwd = 5, col = 'blue')
arrows(0,0, Q[,2][1], Q[,2][2], length = 0.3,
angle = 8, lwd = 2, col = 'red',  lty = 3)
plot(9,9,
main = 'An eigenvector vs a non-eigenvector',
cex.axis = 1.4, cex.lab = 1.4,
xlim = c(-1.2,1.2), ylim=c(-1.2,1.2),
xlab = bquote(x[1]), ylab = bquote(x[2]))
arrows(0,0, Q[,1][1], Q[,1][2], length = 0.25,
angle = 8, lwd = 5, col = 'blue')
arrows(0,0, Q[,2][1], Q[,2][2], length = 0.3,
angle = 8, lwd = 2, col = 'red',  lty = 3)
t = seq(0, 2*pi, len = 101)
x = cos(t)
y = sin(t)
lines(x,y, type = 'o', cex = 0.3)
D = t(Q)%*%C%*%Q #Matrix diagonalization
plot(9,9,
main = 'An eigenvector vs a non-eigenvector',
cex.axis = 1.4, cex.lab = 1.4,
xlim = c(-1.2,1.2), ylim=c(-1.2,1.2),
xlab = bquote(x[1]), ylab = bquote(x[2]))
arrows(0,0, Q[,1][1], Q[,1][2], length = 0.25,
angle = 8, lwd = 5, col = 'blue')
arrows(0,0, Q[,2][1], Q[,2][2], length = 0.3,
angle = 8, lwd = 2, col = 'red',  lty = 3)
t = seq(0, 2*pi, len = 101)
x = cos(t)
y = sin(t)
lines(x,y, type = 'o', cex = 0.3)
text(0.4,0.4, 'Evector1', col ='blue')
text(-0.4,0.3, 'Evector2', col ='red')
text(0, -0.1, '0', col = 'black')
D = t(Q)%*%C%*%Q #Matrix diagonalization
# Problem 10
n <- 5  # space dimension (N)
m <- 4  # time dimension (Y)
set.seed(123)
A <- matrix(rnorm(n*m), nrow=n)
# Step 1: Perform SVD on A
svd_A <- svd(A)
U <- svd_A$u
D <- diag(svd_A$d)
V <- svd_A$v
# Step 2: Calculate covariance matrix C = (1/Y)AA^t
Y <- ncol(A)  # time dimension
C <- (1/Y) * A %*% t(A)
# Step 3: Calculate eigendecomposition of C
eigen_C <- eigen(C)
cat("Proof Verification:\n")
cat("\n1. Relationship between eigenvectors:\n")
# Compare U from SVD with eigenvectors of C
print(all.equal(abs(U), abs(eigen_C$vectors)))
cat("\n2. Relationship between eigenvalues:\n")
# Compare λₖ = dₖ²/Y
theoretical_lambda <- svd_A$d^2/Y
actual_lambda <- eigen_C$values
print(all.equal(theoretical_lambda, actual_lambda))
# Theoretical Proof:
cat("\nTheoretical Proof:\n")
cat("1. Start with SVD: A = UDV^t\n")
cat("2. Form covariance matrix C = (1/Y)AA^t\n")
cat("3. Substitute SVD: C = (1/Y)UDV^t(UDV^t)^t\n")
cat("4. Simplify: C = (1/Y)UDV^tVDU^t\n")
cat("5. Use V^tV = I: C = (1/Y)UD^2U^t\n")
cat("6. For eigenvector u_k: Cu_k = (1/Y)UD^2U^tu_k\n")
cat("7. Using U^tU = I: Cu_k = (1/Y)d_k^2u_k = λ_ku_k\n")
#R plot of Fig. 9.11: A tree in a random forest
#install.packages('rpart')
library(rpart)
#R plot of Fig. 9.11: A tree in a random forest
#install.packages('rpart')
library(rpart)
#install.packages('rpart.plot')
library(rpart.plot)
setwd('~/climstats')
setEPS() #Plot the data of 150 observations
postscript("fig0911.eps",  width=6, height=6)
par(mar = c(0, 2, 1, 1))
iris_tree = rpart( Species ~. , data = train_data)
rpart.plot(iris_tree,
main = 'An decision tree for the RF training data')
dev.off()
#
#
#R code for NN recruitment decision and Fig. 9.12
TKS = c(20,10,30,20,80,30)
CSS = c(90,20,40,50,50,80)
Recruited = c(1,0,0,0,1,1)
# Here, you will combine multiple columns or features into a single set of data
df = data.frame(TKS, CSS, Recruited)
require(neuralnet) # load 'neuralnet' library
# fit neural network
set.seed(123)
nn = neuralnet(Recruited ~ TKS + CSS, data = df,
hidden=5, act.fct = "logistic",
linear.output = FALSE)
plot(nn) #Plot Fig 9.12: A neural network
TKS=c(30,51,72) #new data for decision
CSS=c(85,51,30) #new data for decision
test=data.frame(TKS,CSS)
Predict=neuralnet::compute(nn,test)
Predict$net.result #the result is probability
# Converting probabilities into decisions
ifelse(Predict$net.result > 0.5, 1, 0) #threshold = 0.5
#print bias and weights
nn$weights[[1]][[1]]
#print the last bias and weights before decision
nn$weights[[1]][[2]]
#print the random start bias and weights
nn$startweights
#print error and other technical indices of the nn run
nn$result.matrix #results data
#
#
#R plot Fig. 9.13: Curve of a logistic function
z = seq(-2, 4, len = 101)
k = 3.2
z0 = 1
setEPS() #Automatically saves the .eps file
postscript("fig0913.eps", height=5, width=7)
par(mar = c(4.2, 4.2, 2, 0.5))
plot(z, 1/(1 + exp(-k*(z - z0))),
type = 'l', col = 'blue', lwd =2,
xlab = 'z', ylab = 'g(z)',
main = bquote('Logistic function for k = 3.2,'~z[0]~'= 1.0'),
cex.lab = 1.3, cex.axis = 1.3)
dev.off()
#
#
#R NN code for the Fisher iris flower data
#Ref: https://rpubs.com/vitorhs/iris
data(iris) #150-by-5 iris data
#attach True or False columns to iris data
iris$setosa = iris$Species == "setosa"
iris$virginica = iris$Species == "virginica"
iris$versicolor = iris$Species == "versicolor"
p = 0.5 # assign 50% of data for training
train.idx = sample(x = nrow(iris), size = p*nrow(iris))
train = iris[train.idx,] #determine the training data
test = iris[-train.idx,] #determine the test data
dim(train) #check the train data dimension
#training a neural network
library(neuralnet)
#use the length, width, True and False data for training
iris.nn = neuralnet(setosa + versicolor + virginica ~
Sepal.Length + Sepal.Width +
Petal.Length + Petal.Width,
data = train, hidden=c(10, 10),
rep = 5, err.fct = "ce",
linear.output = F, lifesign = "minimal",
stepmax = 1000000, threshold = 0.001)
plot(iris.nn, rep="best") #plot the neural network
#Prediction for the rest data
prediction = neuralnet::compute(iris.nn, test[,1:4])
#prediction$net.result is 75-by-3 matrix
prediction$net.result[1:3,] #print the first 3 rows
#find which column is for the max of each row
pred.idx <- apply(prediction$net.result, 1, which.max)
pred.idx[70:75] #The last 6 rows
#Assign 1 for setosa, 2 for versicolor, 3 for virginica
predicted <- c('setosa', 'versicolor', 'virginica')[pred.idx]
help()
source("~/Desktop/Education/SDSU/MATH-524 Linear Algebra/Final/alexvo.R")
# Alex Vo
# Assignment 3 - MATH 524
setwd('/Users/alexvo/Desktop/Education/SDSU/MATH-524 Linear Algebra/HW3')
getwd()
clearPushBack()
clear
exit
cls
# Alex Vo
# Assignment 3 - MATH 524
setwd('/Users/alexvo/Desktop/Education/SDSU/MATH-524 Linear Algebra/HW3')
getwd()
library(e1071)
library(dplyr)
library(ggplot2)
library(randomForest)
library(caret)
library(nnet)
getwd()
install.packages("ggplot2")
getwd()
library(e1071)
library(dplyr)
library(ggplot2)
library(randomForest)
library(caret)
library(nnet)
##### 3.1
points <- data.frame(
x = c(1, 2, 2, 3, 4),
y = c(1, 2, 3, 4, 4)
)
update.packages(ask = FALSE, checkBuilt = TRUE)
yes
Yes
update.packages(ask = FALSE, checkBuilt = TRUE)
# Alex Vo
# Assignment 3 - MATH 524
setwd('/Users/alexvo/Desktop/Education/SDSU/MATH-524 Linear Algebra/HW3')
getwd()
library(e1071)# Machine Learning algorithms (with SVM, Naive Bayes, Clustering, Tuning models)
library(dplyr) # Data manipulation (with Filtering, Cleaning, reshaping)
library(ggplot2) # Data Visualization (with Scatterplots, line charts, bar charts, histograms, boxplots, etc.)
library(randomForest) # Classification & Regression using the Random Forest algorithm
library(caret) # Tuning and training many types of models
library(nnet) # Neural Networks package (basic ones)
update.packages(ask = FALSE, checkBuilt = TRUE)
##### 3.1
points <- data.frame(
x = c(1, 2, 2, 3, 4),
y = c(1, 2, 3, 4, 4)
)
set.seed(123)
kmeans_result <- kmeans(points, centers = 2)
# Calculate total within-cluster sum of squares (total)
total <- kmeans_result$tot.withinss
plot(points$x, points$y,
col = kmeans_result$cluster,  # Color points by cluster
pch = 16,                     # Solid circles for points
cex = 2,                      # Size of points
xlim = c(0, 5),              # X-axis limits
ylim = c(0, 5),              # Y-axis limits
xlab = "X",
ylab = "Y",
main = "K-means Clustering (K=2)")
# Add cluster centers
points(kmeans_result$centers,
col = 1:2,                  # Colors matching the clusters
pch = 8,                    # Star symbol for centers
cex = 2)                    # Size of centers
# Add legend
legend("topleft",
legend = c("Cluster 1", "Cluster 2", "Centroids"),
col = c(1, 2, 1),
pch = c(16, 16, 8))
cat("\nCluster Assignments:\n")
print(kmeans_result$cluster)
cat("\nTotal Within-Cluster Sum of Squares (total):", total, "\n")
##### 3.3
data <- read.csv("MiamiIntlAirport2001_2020.csv")
data$DATE <- as.Date(data$DATE) # Convert DATE column to Date type for filtering
# Filter TMIN and WDF2 for 2015
data_2015 <- data %>%
filter(format(DATE, "%Y") == "2015") %>%
select(TMIN, WDF2) %>%
na.omit()  # Remove rows with missing values
head(data_2015)
# Perform K-means clustering with K = 2
set.seed(123)
kmeans_result <- kmeans(data_2015, centers = 2)
# Calculate total within-cluster sum of squares (total)
total <- kmeans_result$tot.withinss
# Calculate total within-cluster sum of squares (total)
total <- kmeans_result$tot.withinss
# Convert cluster centers to a data frame and set column names
centers <- as.data.frame(kmeans_result$centers)
colnames(centers) <- c("TMIN", "WDF2")
ggplot(data_2015, aes(x = TMIN, y = WDF2, color = Cluster)) +
geom_point(size = 3) +
geom_point(data = centers,
aes(x = TMIN, y = WDF2), color = "red", size = 5, shape = 8) +
labs(title = "K-means Clustering of TMIN and WDF2 (K=2)",
x = "TMIN (Daily Minimum Temperature)",
y = "WDF2 (Fastest 2-Minute Wind Direction)") +
theme_minimal() + theme(legend.position = "top")
cat("Total Within-Cluster Sum of Squares (total):", total, "\n")
rlang::last_trace()
rlang::last_trace(drop = FALSE)
##### 3.3
data <- read.csv("MiamiIntlAirport2001_2020.csv")
data$DATE <- as.Date(data$DATE) # Convert DATE column to Date type for filtering
# Filter TMIN and WDF2 for 2015
data_2015 <- data %>%
filter(format(DATE, "%Y") == "2015") %>%
select(TMIN, WDF2) %>%
na.omit()  # Remove rows with missing values
head(data_2015)
# Perform K-means clustering with K = 2
set.seed(123)
kmeans_result <- kmeans(data_2015, centers = 2)
# Calculate total within-cluster sum of squares (total)
total <- kmeans_result$tot.withinss
# Convert cluster centers to a data frame and set column names
centers <- as.data.frame(kmeans_result$centers)
colnames(centers) <- c("TMIN", "WDF2")
ggplot(data_2015, aes(x = TMIN, y = WDF2, color = Cluster)) +
geom_point(size = 3) +
geom_point(data = centers,
aes(x = TMIN, y = WDF2), color = "red", size = 5, shape = 8) +
labs(title = "K-means Clustering of TMIN and WDF2 (K=2)",
x = "TMIN (Daily Minimum Temperature)",
y = "WDF2 (Fastest 2-Minute Wind Direction)") +
theme_minimal() + theme(legend.position = "top")
##### 3.3
data <- read.csv("MiamiIntlAirport2001_2020.csv")
data$DATE <- as.Date(data$DATE) # Convert DATE column to Date type for filtering
# Filter TMIN and WDF2 for 2015
data_2015 <- data %>%
filter(format(DATE, "%Y") == "2015") %>%
select(TMIN, WDF2) %>%
na.omit()  # Remove rows with missing values
head(data_2015)
# Perform K-means clustering with K = 2
set.seed(123)
kmeans_result <- kmeans(data_2015, centers = 2)
# Calculate total within-cluster sum of squares (total)
total <- kmeans_result$tot.withinss
# Add cluster assignments to the data
data_2015$Cluster <- as.factor(kmeans_result$cluster)
# Convert cluster centers to a data frame and set column names
centers <- as.data.frame(kmeans_result$centers)
colnames(centers) <- c("TMIN", "WDF2")
ggplot(data_2015, aes(x = TMIN, y = WDF2, color = Cluster)) +
geom_point(size = 3) +
geom_point(data = centers,
aes(x = TMIN, y = WDF2), color = "red", size = 5, shape = 8) +
labs(title = "K-means Clustering of TMIN and WDF2 (K=2)",
x = "TMIN (Daily Minimum Temperature)",
y = "WDF2 (Fastest 2-Minute Wind Direction)") +
theme_minimal() + theme(legend.position = "top")
cat("Total Within-Cluster Sum of Squares (total):", total, "\n")
##### 3.3
data <- read.csv("MiamiIntlAirport2001_2020.csv")
data$DATE <- as.Date(data$DATE) # Convert DATE column to Date type for filtering
# Filter TMIN and WDF2 for 2015
data_2015 <- data %>%
filter(format(DATE, "%Y") == "2015") %>%
select(TMIN, WDF2) %>%
na.omit()  # Remove rows with missing values
head(data_2015)
# Perform K-means clustering with K = 2
set.seed(123)
kmeans_result <- kmeans(data_2015, centers = 2)
# Calculate total within-cluster sum of squares (total)
total <- kmeans_result$tot.withinss
# Add cluster assignments to the data
data_2015$Cluster <- as.factor(kmeans_result$cluster)
# Convert cluster centers to a data frame and set column names
centers <- as.data.frame(kmeans_result$centers)
colnames(centers) <- c("TMIN", "WDF2")
ggplot(data_2015, aes(x = TMIN, y = WDF2, color = Cluster)) +
geom_point(size = 3) +
geom_point(data = centers,
aes(x = TMIN, y = WDF2), color = "red", size = 5, shape = 8) +
labs(title = "K-means Clustering of TMIN and WDF2 (K=2)",
x = "TMIN (Daily Minimum Temperature)",
y = "WDF2 (Fastest 2-Minute Wind Direction)") +
theme_minimal() + theme(legend.position = "top")
